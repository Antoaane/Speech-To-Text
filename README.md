# Speech-To-Text avec Whisper et PyTorch

Ce projet utilise le mod√®le **Whisper** d'OpenAI pour effectuer la reconnaissance vocale en temps r√©el.

## üöÄ Installation

1. **Cloner le d√©p√¥t** :
   ```bash
   git clone https://github.com/ton-utilisateur/speech-to-text.git
   cd speech-to-text
   ```

2. **Cr√©er un environnement virtuel** :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   .\venv\Scripts\activate   # Windows
   ```

3. **Installer les d√©pendances** :
   ```bash
   pip install -r requirements.txt
   ```

4. **Installer FFmpeg** :
   - T√©l√©charge et installe FFmpeg depuis le site officiel : https://ffmpeg.org/download.html
   - Ajoute FFmpeg au PATH de ton syst√®me.

5. **T√©l√©charger les mod√®les Whisper** :
   - T√©l√©charge le mod√®le souhait√© depuis [Hugging Face](https://huggingface.co/openai/whisper).
   - Place le fichier du mod√®le dans un dossier `models/` √† la racine du projet.

## üßë‚Äçüíª Utilisation

1. **Lancer le service de reconnaissance vocale** :
   ```bash
   python stt.py
   ```

2. Parle dans le micro pour d√©clencher la reconnaissance vocale. Le mot-cl√© d√©fini dans le script (par d√©faut "activation") d√©clenchera l'envoi du texte au serveur Node.js.

## üìÇ Arborescence du projet

```
speech-to-text/
‚îú‚îÄ‚îÄ models/               # Dossier pour les fichiers de mod√®les
‚îú‚îÄ‚îÄ venv/                 # Environnement virtuel (non inclus dans le d√©p√¥t)
‚îú‚îÄ‚îÄ stt.py                # Script principal
‚îú‚îÄ‚îÄ requirements.txt      # D√©pendances du projet
‚îú‚îÄ‚îÄ .gitignore            # Fichiers/dossiers √† ignorer par Git
‚îú‚îÄ‚îÄ README.md             # Documentation du projet
```

## ‚öôÔ∏è Configuration

- **Mot-cl√©** : Le mot-cl√© utilis√© pour d√©tecter les phrases importantes est d√©fini par la variable `KEYWORD` dans le fichier `stt.py`. Par d√©faut, c'est "activation".
- **Endpoint Node.js** : Le serveur Node.js doit √©couter sur l'URL d√©finie dans `NODE_ENDPOINT` (par d√©faut : `http://localhost:3000/keyword-detected`).
- **Mod√®le Whisper** : Tu peux choisir un mod√®le diff√©rent (par exemple, "tiny", "base", "small", "medium", "large") en modifiant la variable `MODEL_NAME` dans le script.

## ‚ú® Am√©liorations possibles

1. **Gestion des pauses** :
   - Actuellement, les pauses sont g√©r√©es par `PAUSE_THRESHOLD`. Ce param√®tre peut √™tre ajust√© pour une meilleure pr√©cision selon ton environnement sonore.

2. **Am√©lioration des performances** :
   - Utiliser une GPU compatible avec CUDA pour acc√©l√©rer la transcription. Assure-toi que PyTorch est install√© avec le support GPU.

3. **Sauvegarde des transcriptions** :
   - Ajouter une fonctionnalit√© pour enregistrer les transcriptions dans un fichier texte ou une base de donn√©es.

4. **Interface utilisateur** :
   - D√©velopper une interface graphique pour surveiller et interagir avec les transcriptions en temps r√©el.

## üìú Licence
Ce projet est distribu√© sous la licence MIT. Consulte le fichier `LICENSE` pour plus d'informations.

---

Pour toute question ou contribution, n'h√©site pas √† ouvrir une issue ou une pull request sur le d√©p√¥t GitHub.
